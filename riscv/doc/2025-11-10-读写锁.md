
# 读写自旋锁实现文档

考虑 xv6 的 sys_pause 和 sys_uptime 函数，它们读取全局 ticks 变量。由于该变量可能被 clockintr 并发更新，这两个函数在读取 ticks 之前获取 tickslock 自旋锁。重要的是，这防止了 clockintr 并发修改 ticks ，但这也阻止了多个核心同时读取 ticks ，这本来是可以接受的。在后一种情况下，自旋锁不必要地降低了性能。

读写锁可能存在的一个微妙问题是，如果有许多读者，那么写者可能永远没有机会运行。换句话说，尽管读者不断获取和释放锁，但实际存在零个读者的时刻永远不会到来，因此写者可能永远无法获取锁。为了解决这个问题，读写锁通常会实现一个写者优先级方案：一旦写者尝试获取锁，后续的读者必须等待写者成功获取并释放锁（当然，在允许写者获取锁之前，需要等待当前读者释放锁）。

## 概述

本实现提供了一个读写自旋锁（rwspinlock），允许多个读者同时访问共享资源，但写者具有排他访问权限。该实现使用原子操作而非传统的锁机制，避免了死锁问题，并确保写者优先级策略。

## 数据结构

```c
struct rwspinlock {
  struct spinlock lk;    // 内部自旋锁，用于保护状态
  uint32 state;          // 锁状态：bit 0 = 写锁，bits 1-31 = 读者计数
  uint32 writers;        // 等待的写者数量
  char *name;            // 锁的名称
};
```

## 状态表示

- `state` 字段使用位模式表示锁的状态：
  - 位 0（最低位）：写锁状态（1 表示写锁被持有）
  - 位 1-31：读者计数（每次增加读者计数加 2，即 10b）
- `writers` 字段表示当前等待获取写锁的写者数量

## 核心函数

### 1. 读者获取锁 (`read_acquire_inner`)

```c
static void
read_acquire_inner(struct rwspinlock *rwlk)
{
  uint32 old_state, new_state;
  
  while (1) {
    // 首先检查是否有等待的写者
    uint32 writers_waiting = __atomic_load_n(&rwlk->writers, __ATOMIC_ACQUIRE);
    
    // 如果有等待的写者，必须等待
    if (writers_waiting > 0) {
      // 自旋等待直到没有写者
      do {
        __sync_synchronize(); // 内存屏障
        writers_waiting = __atomic_load_n(&rwlk->writers, __ATOMIC_ACQUIRE);
      } while (writers_waiting > 0);
      continue; // 写者消失后，从头开始
    }
    
    // 检查锁状态
    old_state = __atomic_load_n(&rwlk->state, __ATOMIC_ACQUIRE);
    
    // 如果写锁被持有，等待
    if (old_state & 1) {
      // 自旋等待写锁释放
      do {
        __sync_synchronize(); // 内存屏障
        old_state = __atomic_load_n(&rwlk->state, __ATOMIC_ACQUIRE);
      } while (old_state & 1);
      // 写锁释放后，从头开始检查新的写者
      continue;
    }
    
    // 最终检查是否有新的写者出现
    if (__atomic_load_n(&rwlk->writers, __ATOMIC_ACQUIRE) > 0) {
      continue; // 如果有写者出现，必须等待
    }
    
    // 尝试增加读者计数（加 2）
    new_state = old_state + 2;
    
    // 原子更新状态
    if (__atomic_compare_exchange_n(&rwlk->state, &old_state, new_state, 
                                   0, __ATOMIC_ACQ_REL, __ATOMIC_ACQUIRE)) {
      break; // 成功获取读锁
    }
    
    // 如果失败，检查是否是因为写者出现
    if (__atomic_load_n(&rwlk->writers, __ATOMIC_ACQUIRE) > 0) {
      continue; // 如果有写者出现，从头开始
    }
  }
}
```

### 2. 写者获取锁 (`write_acquire_inner`)

```c
static void
write_acquire_inner(struct rwspinlock *rwlk)
{
  // 增加写者计数以表示意图
  __atomic_fetch_add(&rwlk->writers, 1, __ATOMIC_ACQ_REL);
  
  // 等待锁完全空闲（无读者或写者）
  uint32 old_state;
  do {
    old_state = __atomic_load_n(&rwlk->state, __ATOMIC_ACQUIRE);
    // 自旋等待锁被释放
    while (old_state != 0) {
      __sync_synchronize(); // 内存屏障
      old_state = __atomic_load_n(&rwlk->state, __ATOMIC_ACQUIRE);
    }
  } while (!__atomic_compare_exchange_n(&rwlk->state, &old_state, 1, 
                                        0, __ATOMIC_ACQ_REL, __ATOMIC_ACQUIRE));
  
  // 减少写者计数，因为我们已经获取了锁
  __atomic_fetch_sub(&rwlk->writers, 1, __ATOMIC_ACQ_REL);
}
```

### 3. 读者释放锁 (`read_release_inner`)

```c
static void
read_release_inner(struct rwspinlock *rwlk)
{
  // 使用原子操作安全地减少读者计数
  uint32 old_state, new_state;
  
  do {
    old_state = __atomic_load_n(&rwlk->state, __ATOMIC_ACQUIRE);
    // 减少读者计数（减 2）
    new_state = old_state - 2;
  } while (!__atomic_compare_exchange_n(&rwlk->state, &old_state, new_state, 
                                        0, __ATOMIC_ACQ_REL, __ATOMIC_ACQUIRE));
}
```

### 4. 写者释放锁 (`write_release_inner`)

```c
static void
write_release_inner(struct rwspinlock *rwlk)
{
  // 使用原子操作安全地释放写锁
  __atomic_store_n(&rwlk->state, 0, __ATOMIC_RELEASE);
}
```

## 设计特点

### 1. 原子操作

- 完全使用原子操作（`__atomic_load_n`, `__atomic_store_n`, `__atomic_fetch_add`, `__atomic_fetch_sub`, `__atomic_compare_exchange_n`）
- 避免了在持有锁时调用 `yield()`，从而防止调度器死锁

### 2. 写者优先级

- 读者必须等待所有等待的写者完成
- 即使写锁被释放，如果有新的写者出现，读者也必须等待
- 在原子比较交换失败后，再次检查写者

### 3. 内存屏障

- 使用 `__sync_synchronize()` 确保内存操作的正确顺序
- 使用适当的内存序（`__ATOMIC_ACQUIRE`, `__ATOMIC_RELEASE`, `__ATOMIC_ACQ_REL`）

## 性能考虑

1. **读者并发**：多个读者可以同时持有锁，提高并发性能
2. **写者优先**：写者优先级策略可能导致读者饥饿，但确保写者不会无限期等待
3. **自旋等待**：在锁竞争激烈时，自旋等待可能消耗 CPU 资源

## 使用示例

```c
struct rwspinlock my_lock;

// 初始化锁
initrwlock(&my_lock, "my_lock");

// 读者使用
read_acquire(&my_lock);
// 临界区 - 读取共享资源
read_release(&my_lock);

// 写者使用
write_acquire(&my_lock);
// 临界区 - 修改共享资源
write_release(&my_lock);
```

## 测试

实现通过了以下测试：
1. 并发读者获取锁
2. 并发读者释放锁
3. 写者优先级测试
4. 多个写者优先级测试
5. 多锁获取和释放测试

## 总结

这个读写自旋锁实现通过原子操作避免了死锁问题，同时确保了写者优先级策略。它适用于读者多于写者的场景，能够提供良好的并发性能。


# 2025-11-10 -- 读写锁测试用例不支持重复测试

```bash
$ rwlktest
rwspinlock_test: step 1: initrwlock
rwspinlock_test: step 2: concurrent read_acquire
rwspinlock_test: step 3: concurrent read_release
rwspinlock_test: step 4: prepare read_acquire for writer priority test
rwspinlock_test: step 5: writer priority test
rwspinlock_test: step 6: checking for concurrent readers/writers
rwspinlock_test: step 7: checking for concurrent writers
rwspinlock_test: step 8: acquiring multiple locks
rwspinlock_test: step 9: releasing multiple locks
rwspinlock_test: step 10: prepare read_acquire for multiple writer priority test
rwspinlock_test: step 11: multiple writer priority test
rwspinlock_test: step 12: prepare read_acquire for multiple writer priority test
rwspinlock_test: step 13: multiple writer priority test
rwspinlock_test: step 14: prepare read_acquire for multiple writer priority test
rwspinlock_test: step 15: multiple writer priority test
rwspinlock_test: step 16: prepare read_acquire for multiple writer priority test
rwspinlock_test: step 17: multiple writer priority test
rwspinlock_test: step 18: prepare read_acquire for multiple writer priority test
rwspinlock_test: step 19: multiple writer priority test
rwspinlock_test: step 20: prepare read_acquire for multiple writer priority test
rwspinlock_test: step 21: multiple writer priority test
rwspinlock_test: step 22: prepare read_acquire for multiple writer priority test
rwspinlock_test: step 23: multiple writer priority test
rwspinlock_test: step 24: prepare read_acquire for multiple writer priority test
rwspinlock_test: step 25: multiple writer priority test
rwspinlock_test: step 26: prepare read_acquire for multiple writer priority test
rwspinlock_test: step 27: multiple writer priority test
rwspinlock_test: step 28: prepare read_acquire for multiple writer priority test
rwspinlock_test: step 29: multiple writer priority test
rwspinlock_test: step 30: done
rwspinlock_test(0): 0
rwspinlock_test(3): 0
rwspinlock_test(2): 0
rwspinlock_test(1): 0
rwlktest: 4/4 CPUs succeeded
$ rwlktest
rwspinlock_test: step 1: initrwlock
rwspinlock_test: step 2: concurrent read_acquire
rwspinlock_test: step 3: concurrent read_release
rwspinlock_test: step 4: prepare read_acquire for writer priority test
rwspinlock_test: step 5: writer priority test
```
读写锁bug第一次测试通过，第二次测试失败，原因不明。

## 可能的原因，第一次测试都能通过，第二次测试无法通过，测试用例中的静态变量累积了状态，导致第二次测试时状态不一致。目前未发现其他bug，实现是否正确待验证。

从测试代码的设计来看，有几个关键点：

1. 静态变量的使用：
  - barrier变量用于同步所有CPU，每次调用rwspinlock_test_step时递增
  - flag、v、writer_count等变量用于测试不同场景
  - 这些静态变量在测试过程中累积状态，不会被重置
2. 测试流程：

  - 测试分为30个步骤，每个步骤都需要所有CPU同步
  - 测试包括并发读、写者优先级、多个写者优先级等多种场景
  - 每个场景都依赖于前一个场景的状态
3. 死锁位置：
  - 死锁总是发生在第6步"checking for concurrent readers/writers"
  - 这一步测试CPU0执行写操作，其他CPU执行读操作
  - 第二次测试时，可能因为状态不一致导致死锁
从这些观察来看，测试用例很可能设计为只支持单次测试。原因如下：
  - 静态变量累积：测试中的静态变量在每次测试后不会被重置，导致第二次测试时状态不一致。
  - 测试依赖性：测试步骤之间有依赖关系，后续步骤依赖于前面步骤的状态。
  - 资源清理：测试结束后没有进行资源清理，导致第二次测试时资源状态不正确。
  - 因此，我认为这个测试用例的逻辑确实只支持单次测试，不支持多次测试。如果需要多次测试，需要在每次测试开始时重置所有静态变量和资源状态。

这解释了为什么第一次测试能够通过，但第二次测试就会死锁。这并不是读写锁实现的问题，而是测试用例设计的问题。